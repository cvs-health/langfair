% Winogender 
% https://github.com/rudinger/winogender-schemas
@InProceedings{rudinger-EtAl:2018:N18,
  author    = {Rudinger, Rachel  and  Naradowsky, Jason  and  Leonard, Brian  and  {Van Durme}, Benjamin},
  title     = {Gender Bias in Coreference Resolution},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics}
}


%WinoBias
@misc{zhao-2018,
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
	month = {4},
	title = {{Gender Bias in Coreference Resolution: Evaluation and Debiasing methods}},
	year = {2018},
	url = {https://arxiv.org/abs/1804.06876},
}


%GAP
@article{webster-etal-2018-mind,
    title = "Mind the {GAP}: A Balanced Corpus of Gendered Ambiguous Pronouns",
    author = "Webster, Kellie  and
      Recasens, Marta  and
      Axelrod, Vera  and
      Baldridge, Jason",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina  and
      Roark, Brian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1042",
    doi = "10.1162/tacl_a_00240",
    pages = "605--617",
    abstract = "Coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. Nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. Furthermore, we find gender bias in existing corpora and systems favoring masculine entities. To address this, we present and release GAP, a gender-balanced labeled corpus of 8,908 ambiguous pronoun{--}name pairs sampled to provide diverse coverage of challenges posed by real-world text. We explore a range of baselines that demonstrate the complexity of the challenge, the best achieving just 66.9{\%} F1. We show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge.",
}


%BUG
@misc{levy2021collecting,
      title={Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation}, 
      author={Shahar Levy and Koren Lazar and Gabriel Stanovsky},
      year={2021},
      eprint={2109.03858},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


%StereoSet
@misc{nadeem2020stereoset,
    title={StereoSet: Measuring stereotypical bias in pretrained language models},
    author={Moin Nadeem and Anna Bethke and Siva Reddy},
    year={2020},
    eprint={2004.09456},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


%BEC-PRO
@inproceedings{bartl2020unmasking,
  title={Unmasking Contextual Stereotypes: Measuring and Mitigating BERT's Gender Bias},
  author={Bartl, Marion and Nissim, Malvina and Gatt, Albert},
  editor={Costa-jussà, Marta R. and Hardmeier, Christian and Webster, Kellie and Radford, Will},
  booktitle={Proceedings of the Second Workshop on Gender Bias in Natural Language Processing},
  year={2020}
}


%Crows-Pairs
@inproceedings{nangia2020crows,
    title = "{CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models}",
    author = "Nangia, Nikita  and
      Vania, Clara  and
      Bhalerao, Rasika  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics"
}


%WinoQueer
@misc{felkner2024winoqueercommunityintheloopbenchmarkantilgbtq,
      title={WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models}, 
      author={Virginia K. Felkner and Ho-Chun Herbert Chang and Eugene Jang and Jonathan May},
      year={2024},
      eprint={2306.15087},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.15087}, 
}


%RedditBias
@misc{barikeri2021redditbiasrealworldresourcebias,
      title={RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models}, 
      author={Soumya Barikeri and Anne Lauscher and Ivan Vulić and Goran Glavaš},
      year={2021},
      eprint={2106.03521},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.03521}, 
}


%PANDA
@misc{qian2022perturbationaugmentationfairernlp,
      title={Perturbation Augmentation for Fairer NLP}, 
      author={Rebecca Qian and Candace Ross and Jude Fernandes and Eric Smith and Douwe Kiela and Adina Williams},
      year={2022},
      eprint={2205.12586},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.12586}, 
}


%EquityEvaluationCorpus
@misc{kiritchenko2018examininggenderracebias,
      title={Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems}, 
      author={Svetlana Kiritchenko and Saif M. Mohammad},
      year={2018},
      eprint={1805.04508},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1805.04508}, 
}


%RealToxicityPrompts
%https://toxicdegeneration.allenai.org
@inproceedings{Gehman2020RealToxicityPromptsEN,
  title={RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models},
  author={Samuel Gehman and Suchin Gururangan and Maarten Sap and Yejin Choi and Noah A. Smith},
  booktitle={Findings},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:221878771}
}


%BOLD
%https://github.com/amazon-science/bold
@inproceedings{bold_2021,
author = {Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
title = {BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445924},
doi = {10.1145/3442188.3445924},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {862–872},
numpages = {11},
keywords = {natural language generation, Fairness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}


%TrustGPT
@misc{huang2023trustgptbenchmarktrustworthyresponsible,
      title={TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models}, 
      author={Yue Huang and Qihui Zhang and Philip S. Y and Lichao Sun},
      year={2023},
      eprint={2306.11507},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.11507}, 
}


%HONEST
@inproceedings{nozza-etal-2021-honest,
    title = {"{HONEST}: Measuring Hurtful Sentence Completion in Language Models"},
    author = "Nozza, Debora and Bianchi, Federico  and Hovy, Dirk",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.191",
    doi = "10.18653/v1/2021.naacl-main.191",
    pages = "2398--2406",
}


%BBQ
@inproceedings{parrish-etal-2022-bbq,
    title = "{BBQ}: A hand-built bias benchmark for question answering",
    author = "Parrish, Alicia  and
      Chen, Angelica  and
      Nangia, Nikita  and
      Padmakumar, Vishakh  and
      Phang, Jason  and
      Thompson, Jana  and
      Htut, Phu Mon  and
      Bowman, Samuel",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.165",
    doi = "10.18653/v1/2022.findings-acl.165",
    pages = "2086--2105",
    abstract = "It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model{'}s biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model{'}s outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.",
}

%UnQOVER
@inproceedings{li-etal-2020-unqovering,
    title = "{UNQOVER}ing Stereotyping Biases via Underspecified Questions",
    author = "Li, Tao  and
      Khashabi, Daniel  and
      Khot, Tushar  and
      Sabharwal, Ashish  and
      Srikumar, Vivek",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.311",
    doi = "10.18653/v1/2020.findings-emnlp.311",
    pages = "3475--3489",
    abstract = "While language embeddings have been shown to have stereotyping biases, how these biases affect downstream question answering (QA) models remains unexplored. We present UNQOVER, a general framework to probe and quantify biases through underspecified questions. We show that a naive use of model scores can lead to incorrect bias estimates due to two forms of reasoning errors: positional dependence and question independence. We design a formalism that isolates the aforementioned errors. As case studies, we use this metric to analyze four important classes of stereotypes: gender, nationality, ethnicity, and religion. We probe five transformer-based QA models trained on two QA datasets, along with their underlying language models. Our broad study reveals that (1) all these models, with and without fine-tuning, have notable stereotyping biases in these classes; (2) larger models often have higher bias; and (3) the effect of fine-tuning on bias varies strongly with the dataset and the model size.",
}


%Grep_BiasIR
@inproceedings{10.1145/3576840.3578295,
author = {Krieg, Klara and Parada-Cabaleiro, Emilia and Medicus, Gertraud and Lesota, Oleg and Schedl, Markus and Rekabsaz, Navid},
title = {Grep-BiasIR: A Dataset for Investigating Gender Representation Bias in Information Retrieval Results},
year = {2023},
isbn = {9798400700354},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576840.3578295},
doi = {10.1145/3576840.3578295},
abstract = {The provided contents by information retrieval (IR) systems can reflect the existing societal biases and stereotypes. Such biases in retrieval results can lead to further establishing and strengthening stereotypes in society and also in the systems. To facilitate the studies of gender bias in the retrieval results of IR systems, we introduce Gender Representation-Bias for Information Retrieval (Grep-BiasIR), a novel thoroughly-audited dataset consisting of 118 bias-sensitive neutral search queries. The set of queries covers a wide range of gender-related topics, for which a biased representation of genders in the search result can be considered as socially problematic. Each query is accompanied with one relevant and one non-relevant document, where the document is also provided in three variations of female, male, and neutral. The dataset is available at https://github.com/KlaraKrieg/GrepBiasIR.},
booktitle = {Proceedings of the 2023 Conference on Human Information Interaction and Retrieval},
pages = {444–448},
numpages = {5},
keywords = {content bias, dataset, gender bias, information retrieval, representation bias},
location = {Austin, TX, USA},
series = {CHIIR '23}
}


% HELM
@misc{liang2023holisticevaluationlanguagemodels,
      title={Holistic Evaluation of Language Models}, 
      author={Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and Deepak Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Cosgrove and Christopher D. Manning and Christopher Ré and Diana Acosta-Navas and Drew A. Hudson and Eric Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue Wang and Keshav Santhanam and Laurel Orr and Lucia Zheng and Mert Yuksekgonul and Mirac Suzgun and Nathan Kim and Neel Guha and Niladri Chatterji and Omar Khattab and Peter Henderson and Qian Huang and Ryan Chi and Sang Michael Xie and Shibani Santurkar and Surya Ganguli and Tatsunori Hashimoto and Thomas Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda},
      year={2023},
      eprint={2211.09110},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.09110}, 
}


% decodingtrust
@article{wang2023decodingtrust,
  title={DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models},
  author={Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and others},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2023}
}


%evaluate
@misc{huggingface-no-date,
	author = {Huggingface},
	title = {GitHub - huggingface/evaluate: Evaluate: A library for easily evaluating machine learning models and datasets.},
	url = {https://github.com/huggingface/evaluate},
}


% langtest
@article{Arshaan_Nazir_and_Thadaka_Kalyan_Chakravarthy_and_David_Amore_Cecchini_and_Thadaka_Kalyan_Chakravarthy_and_Rakshit_Khajuria_and_Prikshit_Sharma_and_Ali_Tarik_Mirik_and_Veysel_Kocaman_and_David_Talby_LangTest_A_comprehensive_2024,
author = {Arshaan Nazir and Thadaka Kalyan Chakravarthy and David Amore Cecchini and Thadaka Kalyan Chakravarthy and Rakshit Khajuria and Prikshit Sharma and Ali Tarik Mirik and Veysel Kocaman and David Talby},
doi = {10.1016/j.simpa.2024.100619},
journal = {Software Impacts},
number = {100619},
title = {{LangTest: A comprehensive evaluation library for custom LLM and NLP models}},
volume = {19},
year = {2024}
}


%BIG-bench
@article{srivastava2023beyond,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={BIG-bench authors},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2023},
  url={https://openreview.net/forum?id=uyTL5Bvosj},
  note={}
}


%lm-evaluation-harness
@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 07,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.3},
  doi          = {10.5281/zenodo.12608602},
  url          = {https://zenodo.org/records/12608602}
}


%TrustLLM
@inproceedings{huang2024trustllm,
  title={TrustLLM: Trustworthiness in Large Language Models},
  author={Yue Huang and Lichao Sun and Haoran Wang and Siyuan Wu and Qihui Zhang and Yuan Li and Chujie Gao and Yixin Huang and Wenhan Lyu and Yixuan Zhang and Xiner Li and Hanchi Sun and Zhengliang Liu and Yixin Liu and Yijue Wang and Zhikun Zhang and Bertie Vidgen and Bhavya Kailkhura and Caiming Xiong and Chaowei Xiao and Chunyuan Li and Eric P. Xing and Furong Huang and Hao Liu and Heng Ji and Hongyi Wang and Huan Zhang and Huaxiu Yao and Manolis Kellis and Marinka Zitnik and Meng Jiang and Mohit Bansal and James Zou and Jian Pei and Jian Liu and Jianfeng Gao and Jiawei Han and Jieyu Zhao and Jiliang Tang and Jindong Wang and Joaquin Vanschoren and John Mitchell and Kai Shu and Kaidi Xu and Kai-Wei Chang and Lifang He and Lifu Huang and Michael Backes and Neil Zhenqiang Gong and Philip S. Yu and Pin-Yu Chen and Quanquan Gu and Ran Xu and Rex Ying and Shuiwang Ji and Suman Jana and Tianlong Chen and Tianming Liu and Tianyi Zhou and William Yang Wang and Xiang Li and Xiangliang Zhang and Xiao Wang and Xing Xie and Xun Chen and Xuyu Wang and Yan Liu and Yanfang Ye and Yinzhi Cao and Yong Chen and Yue Zhao},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
  url={https://openreview.net/forum?id=bWUU0LwwMp}
}


%ML Fairness 
%AIF360
@misc{aif360-oct-2018,
    title = "{AI Fairness} 360:  An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias",
    author = {Rachel K. E. Bellamy and Kuntal Dey and Michael Hind and
	Samuel C. Hoffman and Stephanie Houde and Kalapriya Kannan and
	Pranay Lohia and Jacquelyn Martino and Sameep Mehta and
	Aleksandra Mojsilovic and Seema Nagar and Karthikeyan Natesan Ramamurthy and
	John Richards and Diptikalyan Saha and Prasanna Sattigeri and
	Moninder Singh and Kush R. Varshney and Yunfeng Zhang},
    month = oct,
    year = {2018},
    url = {https://arxiv.org/abs/1810.01943}
}


%fairlearn
@article{Weerts_Fairlearn_Assessing_and_2023,
author = {Weerts, Hilde and Dudík, Miroslav and Edgar, Richard and Jalali, Adrin and Lutz, Roman and Madaio, Michael},
journal = {Journal of Machine Learning Research},
title = {{Fairlearn: Assessing and Improving Fairness of AI Systems}},
url = {http://jmlr.org/papers/v24/23-0389.html},
volume = {24},
year = {2023}
}


%aequitas
  @article{2018aequitas,
     title={Aequitas: A Bias and Fairness Audit Toolkit},
     author={Saleiro, Pedro and Kuester, Benedict and Stevens, Abby and Anisfeld, Ari and Hinkson, Loren and London, Jesse and Ghani, Rayid}, journal={arXiv preprint arXiv:1811.05577}, year={2018}
}


% What-if-tool
@article{DBLP:journals/corr/abs-1907-04135,
  author       = {James Wexler and
                  Mahima Pushkarna and
                  Tolga Bolukbasi and
                  Martin Wattenberg and
                  Fernanda B. Vi{\'{e}}gas and
                  Jimbo Wilson},
  title        = {The What-If Tool: Interactive Probing of Machine Learning Models},
  journal      = {CoRR},
  volume       = {abs/1907.04135},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.04135},
  eprinttype    = {arXiv},
  eprint       = {1907.04135},
  timestamp    = {Wed, 17 Jul 2019 10:27:36 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-04135.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


%fairness-indicators
@misc{tensorflow-no-date,
	author = {Tensorflow},
	title = {{GitHub - tensorflow/fairness-indicators: Tensorflow's Fairness Evaluation and Visualization Toolkit}},
	url = {https://github.com/tensorflow/fairness-indicators},
}


%LiFT
@inproceedings{vasudevan20lift,
    author       = {Vasudevan, Sriram and Kenthapadi, Krishnaram},
    title        = {{LiFT}: A Scalable Framework for Measuring Fairness in ML Applications},
    booktitle    = {Proceedings of the 29th ACM International Conference on Information and Knowledge Management},
    series       = {CIKM '20},
    year         = {2020},
    pages        = {},
    numpages     = {8}
}



@misc{bouchard2024actionableframeworkassessingbias,
      title={An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases}, 
      author={Dylan Bouchard},
      year={2024},
      eprint={2407.10853},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10853}, 
}


%COBS
@inproceedings{bordia-bowman-2019-identifying,
    title = "Identifying and Reducing Gender Bias in Word-Level Language Models",
    author = "Bordia, Shikha  and
      Bowman, Samuel R.",
    editor = "Kar, Sudipta  and
      Nadeem, Farah  and
      Burdick, Laura  and
      Durrett, Greg  and
      Han, Na-Rae",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-3002",
    doi = "10.18653/v1/N19-3002",
    pages = "7--15",
    abstract = "Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data. For example, doctor cooccurs more frequently with male pronouns than female pronouns. In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases. We replicate this study on three training corpora{---}Penn Treebank, WikiText-2, and CNN/Daily Mail{---}resulting in similar conclusions.",
}


%stereotype classifiers
@misc{zekun2023auditinglargelanguagemodels,
      title={Towards Auditing Large Language Models: Improving Text-based Stereotype Detection}, 
      author={Wu Zekun and Sahan Bulathwela and Adriano Soares Koshiyama},
      year={2023},
      eprint={2311.14126},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.14126}, 
}


% counterfactual fairness
@misc{kusner2018counterfactualfairness,
      title={Counterfactual Fairness}, 
      author={Matt J. Kusner and Joshua R. Loftus and Chris Russell and Ricardo Silva},
      year={2018},
      eprint={1703.06856},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1703.06856}, 
}


% gallegos survey
@misc{gallegos2024biasfairnesslargelanguage,
      title={Bias and Fairness in Large Language Models: A Survey}, 
      author={Isabel O. Gallegos and Ryan A. Rossi and Joe Barrow and Md Mehrab Tanjim and Sungchul Kim and Franck Dernoncourt and Tong Yu and Ruiyi Zhang and Nesreen K. Ahmed},
      year={2024},
      eprint={2309.00770},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.00770}, 
}


% counterfactual sentiment
@misc{huang2020reducingsentimentbiaslanguage,
      title={Reducing Sentiment Bias in Language Models via Counterfactual Evaluation}, 
      author={Po-Sen Huang and Huan Zhang and Ray Jiang and Robert Stanforth and Johannes Welbl and Jack Rae and Vishal Maini and Dani Yogatama and Pushmeet Kohli},
      year={2020},
      eprint={1911.03064},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1911.03064}, 
}


% Rec
@inproceedings{Zhang_2023, series={RecSys ’23},
   title={Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation},
   volume={2012},
   url={http://dx.doi.org/10.1145/3604915.3608860},
   DOI={10.1145/3604915.3608860},
   booktitle={Proceedings of the 17th ACM Conference on Recommender Systems},
   publisher={ACM},
   author={Zhang, Jizhi and Bao, Keqin and Zhang, Yang and Wang, Wenjie and Feng, Fuli and He, Xiangnan},
   year={2023},
   month=sep, pages={993–999},
   collection={RecSys ’23} }


% DI
@misc{feldman2015certifyingremovingdisparateimpact,
      title={Certifying and removing disparate impact}, 
      author={Michael Feldman and Sorelle Friedler and John Moeller and Carlos Scheidegger and Suresh Venkatasubramanian},
      year={2015},
      eprint={1412.3756},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1412.3756}, 
}


@misc{goldfarbtarrant2021intrinsicbiasmetricscorrelate,
      title={Intrinsic Bias Metrics Do Not Correlate with Application Bias}, 
      author={Seraphina Goldfarb-Tarrant and Rebecca Marchant and Ricardo Muñoz Sanchez and Mugdha Pandya and Adam Lopez},
      year={2021},
      eprint={2012.15859},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2012.15859}, 
}


@inproceedings{delobelle-etal-2022-measuring,
    title = "Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models",
    author = "Delobelle, Pieter  and
      Tokpo, Ewoenam  and
      Calders, Toon  and
      Berendt, Bettina",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.122",
    doi = "10.18653/v1/2022.naacl-main.122",
    pages = "1693--1706",
    abstract = "An increasing awareness of biased patterns in natural language processing resources such as BERT has motivated many metrics to quantify {`}bias{'} and {`}fairness{'} in these resources. However, comparing the results of different metrics and the works that evaluate with such metrics remains difficult, if not outright impossible. We survey the literature on fairness metrics for pre-trained language models and experimentally evaluate compatibility, including both biases in language models and in their downstream tasks. We do this by combining traditional literature survey, correlation analysis and empirical evaluations. We find that many metrics are not compatible with each other and highly depend on (i) templates, (ii) attribute and target seeds and (iii) the choice of embeddings. We also see no tangible evidence of intrinsic bias relating to extrinsic bias. These results indicate that fairness or bias evaluation remains challenging for contextualized language models, among other reasons because these choices remain subjective. To improve future comparisons and fairness evaluations, we recommend to avoid embedding-based metrics and focus on fairness evaluations in downstream tasks.",
}

% Evaluate
@misc{huggingface-no-date,
	author = {Huggingface},
	title = {GitHub - huggingface/evaluate: Evaluate: A library for easily evaluating machine learning models and datasets.},
	url = {https://github.com/huggingface/evaluate},
}