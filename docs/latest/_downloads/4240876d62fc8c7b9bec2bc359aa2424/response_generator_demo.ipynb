{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# ``ResponseGenerator`` Class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import necessary libraries for the notebook.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Run if python-dotenv not installed\n# import sys\n# !{sys.executable} -m pip install python-dotenv\n\nimport json\nimport os\nimport time\n\nimport openai\nimport pandas as pd\nfrom dotenv import load_dotenv\n\nfrom langfair.generator import ResponseGenerator\n\n# User to populate .env file with API credentials\nrepo_path = \"/\".join(os.getcwd().split(\"/\")[:-2])\nload_dotenv(os.path.join(repo_path, \".env\"))\n\nAPI_KEY = os.getenv(\"API_KEY\")\nAPI_BASE = os.getenv(\"API_BASE\")\nAPI_TYPE = os.getenv(\"API_TYPE\")\nAPI_VERSION = os.getenv(\"API_VERSION\")\nMODEL_VERSION = os.getenv(\"MODEL_VERSION\")\nDEPLOYMENT_NAME = os.getenv(\"DEPLOYMENT_NAME\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read in prompts from which responses will be generated.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# THIS IS AN EXAMPLE SET OF PROMPTS\nresource_path = os.path.join(repo_path, \"data/RealToxicityPrompts.jsonl\")\nwith open(resource_path, \"r\") as file:\n    # Read each line in the file\n    challenging = []\n    prompts = []\n    for line in file:\n        # Parse the JSON object from each line\n        challenging.append(json.loads(line)[\"challenging\"])\n        prompts.append(json.loads(line)[\"prompt\"][\"text\"])\nprompts = [prompts[i] for i in range(len(prompts)) if not challenging[i]][0:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``ResponseGenerator()`` - Class for generating data for evaluation from provided set of prompts (class)\n\nClass parameters:\n\n- ``langchain_llm`` (**langchain llm (Runnable), default=None**) A langchain llm object to get passed to LLMChain `llm` argument.\n- ``suppressed_exceptions`` (**tuple, default=None**) Specifies which exceptions to handle as 'Unable to get response' rather than raising the exception\n- ``max_calls_per_min`` (**Deprecated as of 0.2.0**) Use LangChain's InMemoryRateLimiter instead.\n\nBelow we use LangFair's ``ResponseGenerator`` class to generate LLM responses. To instantiate the ``ResponseGenerator`` class, pass a LangChain LLM object as an argument. Note that although this notebook uses ``AzureChatOpenAI``, this can be replaced with a LangChain LLM of your choice.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# # Run if langchain-openai not installed\n# import sys\n# !{sys.executable} -m pip install langchain-openai\n\n# Example with AzureChatOpenAI. REPLACE WITH YOUR LLM OF CHOICE.\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    deployment_name=DEPLOYMENT_NAME,\n    openai_api_key=API_KEY,\n    azure_endpoint=API_BASE,\n    openai_api_type=API_TYPE,\n    openai_api_version=API_VERSION,\n    temperature=1,  # User to set temperature\n)\n\n\n# Create langfair ResponseGenerator object\nrg = ResponseGenerator(\n    langchain_llm=llm,\n    suppressed_exceptions=(\n        openai.BadRequestError,\n        ValueError,\n    ),  # this suppresses content filtering errors\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Estimate token costs before generation**\n\n``estimate_token_cost()`` - Estimates the token cost for a given list of prompts and (optionally) example responses. This method is only compatible with GPT models.\n\n Method Parameters:\n\n  - ``prompts`` - (**list of strings**) A list of prompts.\n  - ``example_responses`` - (**list of strings, optional**) A list of example responses. If provided, the function will estimate the response tokens based on these examples.\n  - ``model_name`` - (**str, optional**) The name of the OpenAI model to use for token counting.\n  - ``response_sample_size`` - (**int, default=30**) The number of responses to generate for cost estimation if `response_example_list` is not provided.\n  - ``system_prompt`` - (**str, default=\"You are a helpful assistant.\"**) The system prompt to use.\n  - ``count`` - (**int, default=25**) The number of generations per prompt used when estimating cost.\n\nReturns:\n- A dictionary containing the estimated token costs, including prompt token cost, completion token cost, and total token cost. (**dictionary**)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for model_name in [\"gpt-3.5-turbo-16k-0613\", \"gpt-4-32k-0613\"]:\n    estimated_cost = await rg.estimate_token_cost(\n        tiktoken_model_name=model_name, prompts=prompts, count=1\n    )\n    print(\n        f\"Estimated cost for {model_name}: $\",\n        round(estimated_cost[\"Estimated Total Token Cost (USD)\"], 2),\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Note that using GPT-4 is considerably more expensive than GPT-3.5</p></div>\n\n## Evaluating Response Time: Asynchronous Generation with ``ResponseGenerator`` vs Synchronous Generation with ``openai.chat.completions.create``\n\nGenerate responses asynchronously with ``ResponseGenerator``\n\n``generate_responses()`` -  Generates evaluation dataset from a provided set of prompts. For each prompt, ``self.count`` responses are generated.\nMethod Parameters:\n\n- ``prompts`` - (**list of strings**) A list of prompts\n- ``system_prompt`` - (**str or None, default=\"You are a helpful assistant.\"**) Specifies the system prompt used when generating LLM responses.\n- ``count`` - (**int, default=25**) Specifies number of responses to generate for each prompt.\n\nReturns:\nA dictionary with two keys: ``data`` and ``metadata``.\n- ``data`` (**dict**) A dictionary containing the prompts and responses.\n- ``metadata`` (**dict**) A dictionary containing metadata about the generation process, including non-completion rate, temperature, and count.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate 1 response per prompt for 200 prompts\nstart = time.time()\nasync_responses = await rg.generate_responses(prompts=prompts[0:200], count=1)\nstop = time.time()\nprint(f\"Time elapsed for asynchronous generation: {stop - start}\")\n\npd.DataFrame(async_responses[\"data\"])\n\nasync_responses[\"metadata\"]\n\n\n# Generate responses synchronously for comparison\n\n\ndef openai_api_call(\n    prompt, system_prompt=\"You are a helpful assistant.\", model=\"exai-gpt-35-turbo-16k\"\n):\n    try:\n        completion = openai.chat.completions.create(\n            model=model,\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n        )\n        return completion.choices[0].message.content\n    except openai.BadRequestError:\n        return \"Unable to get response\"\n\n\nopenai.api_key = API_KEY\nopenai.azure_endpoint = API_BASE\nopenai.model_version = MODEL_VERSION\nopenai.api_version = API_VERSION\nopenai.api_type = API_TYPE\n\nstart = time.time()\nsync_responses = [openai_api_call(prompt) for prompt in prompts[0:200]]\nstop = time.time()\nprint(f\"Time elapsed for synchronous generation: {stop - start}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that asynchronous generation with `ResponseGenerator` is significantly faster than synchonous generation.\n\nHandling ``RateLimitError`` with ``ResponseGenerator``\n\nPassing too many requests asynchronously will trigger a ``RateLimitError``. For our '`exai-gpt-35-turbo-16k`' deployment, 1000 prompts at 25 generations per prompt with async exceeds the rate limit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "responses = await rg.generate_responses(prompts=prompts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To handle this error, we can use LangChain's ``InMemoryRateLimiter`` to limit the number of requests per minute.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from langchain_core.rate_limiters import InMemoryRateLimiter\n\nrate_limiter = InMemoryRateLimiter(\n    requests_per_second=5,\n    check_every_n_seconds=5,\n    max_bucket_size=500,\n)\n\nllm = AzureChatOpenAI(\n    deployment_name=DEPLOYMENT_NAME,\n    openai_api_key=API_KEY,\n    azure_endpoint=API_BASE,\n    openai_api_type=API_TYPE,\n    openai_api_version=API_VERSION,\n    temperature=1,  # User to set temperature\n    rate_limiter=rate_limiter,\n)\n\nrg_limited = ResponseGenerator(langchain_llm=llm)\n\nresponses = await rg_limited.generate_responses(prompts=prompts)\n\npd.DataFrame(responses[\"data\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}