{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Toxicity Metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>Due to the topic of bias and fairness, some users may be offended by the content contained herein, including prompts and output generated from use of the prompts.</p></div>\n\n## Content\n\n1. `Introduction<intro>`\n\n2. `Generate Evaluation Dataset<gen-evaluation-dataset>`\n\n3. `Assessment<assessment>`\n\n4. `Metric Definitions<metric-defns>`\n\nImport necessary libraries for the notebook.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Run if python-dotenv not installed\n# import sys\n# !{sys.executable} -m pip install python-dotenv\n\nimport json\nimport os\n\nimport pandas as pd\nfrom dotenv import find_dotenv, load_dotenv\nfrom langchain_core.rate_limiters import InMemoryRateLimiter\n\nfrom langfair.generator import ResponseGenerator\nfrom langfair.metrics.toxicity import ToxicityMetrics\n\n# User to populate .env file with API credentials\nrepo_path = \"/\".join(os.getcwd().split(\"/\")[:-3])\nload_dotenv(find_dotenv())\n\nAPI_KEY = os.getenv(\"API_KEY\")\nAPI_BASE = os.getenv(\"API_BASE\")\nAPI_TYPE = os.getenv(\"API_TYPE\")\nAPI_VERSION = os.getenv(\"API_VERSION\")\nMODEL_VERSION = os.getenv(\"MODEL_VERSION\")\nDEPLOYMENT_NAME = os.getenv(\"DEPLOYMENT_NAME\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Introduction\n----------------\n\nToxicity in large language model (LLM) outputs refers to offensive language that 1) launches attacks, issues threats, or\nincites hate or violence against a social group, or 2) includes the usage of pejorative slurs, insults, or any other forms of\nexpression that specifically target and belittle a social group. LangFair offers the following toxicity metrics from the LLM fairness literature:\n\n* Expected Maximum Toxicity [Gehman et al., 2020](https://arxiv.org/pdf/2009.11462)\n* Toxicity Probability [Gehman et al., 2020](https://arxiv.org/pdf/2009.11462)\n* Toxic Fraction [Liang et al., 2023](https://arxiv.org/pdf/2211.09110)\n\nFor more details on the definitions of these metrics, refer to the [metric definitions](#section4') in this notebook or LangFair's [technical playbook](https://arxiv.org/pdf/2407.10853).\n\n2. Generate Evaluation Dataset (skip if responses already generated)\n----------------\n\nHere, we will use ``ResponseGenerator`` to generate a large sample of responses with our LLM of choice. The user should replace our example prompts with actual prompts from their use case. **If the user already has a large number of responses generated, they may skip this step.**\n\nRead in prompts\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# THIS IS AN EXAMPLE SET OF PROMPTS. USER TO REPLACE WITH THEIR OWN PROMPTS\nresource_path = os.path.join(repo_path, \"data/RealToxicityPrompts.jsonl\")\nwith open(resource_path, \"r\") as file:\n    # Read each line in the file\n    challenging = []\n    prompts = []\n    for line in file:\n        # Parse the JSON object from each line\n        challenging.append(json.loads(line)[\"challenging\"])\n        prompts.append(json.loads(line)[\"prompt\"][\"text\"])\nprompts = [prompts[i] for i in range(len(prompts)) if not challenging[i]][0:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that sample size is intentionally kept low to reduce execution time of this notebook. User should use all the available propmpts and can use `ResponseGenerator` class to generate more response from a model.\n\nEvaluation Dataset Generation\n\n``ResponseGenerator()`` - Class for generating data for evaluation from provided set of prompts (class)\n\nClass parameters:\n\n  - ``langchain_llm`` (**langchain llm (Runnable), default=None**) A langchain llm object to get passed to LLMChain `llm` argument.\n  - ``suppressed_exceptions`` (**tuple, default=None**) Specifies which exceptions to handle as 'Unable to get response' rather than raising the exception\n  - ``max_calls_per_min`` (**Deprecated as of 0.2.0**) Use LangChain's InMemoryRateLimiter instead.\n\nMethods:\n\n``generate_responses()`` -  Generates evaluation dataset from a provided set of prompts. For each prompt, `self.count` responses are generated.\nMethod Parameters:\n\n- ``prompts`` - (**list of strings**) A list of prompts\n- ``system_prompt`` - (**str or None, default=\"You are a helpful assistant.\"**) Specifies the system prompt used when generating LLM responses.\n- ``count`` - (**int, default=25**) Specifies number of responses to generate for each prompt.\n\nReturns:\nA dictionary with two keys: ``data`` and ``metadata``.\n- ``data`` (**dict**) A dictionary containing the prompts and responses.\n- ``metadata`` (**dict**) A dictionary containing metadata about the generation process, including non-completion rate, temperature, and count.\n\nBelow we use LangFair's `ResponseGenerator` class to generate LLM responses, which will be used to compute evaluation metrics. To instantiate the `ResponseGenerator` class, pass a LangChain LLM object as an argument.\n\n**Important note: We provide three examples of LangChain LLMs below, but these can be replaced with a LangChain LLM of your choice.**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Use LangChain's InMemoryRateLimiter to avoid rate limit errors. Adjust parameters as necessary.\nrate_limiter = InMemoryRateLimiter(\n    requests_per_second=10,\n    check_every_n_seconds=10,\n    max_bucket_size=1000,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example 1: Gemini Pro with VertexAI**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# # Run if langchain-google-vertexai not installed. Note: kernel restart may be required.\n# import sys\n# !{sys.executable} -m pip install langchain-google-vertexai\n\n# from langchain_google_vertexai import VertexAI\n# llm = VertexAI(model_name='gemini-pro', temperature=1, rate_limiter=rate_limiter)\n\n# # Define exceptions to suppress\n# suppressed_exceptions = (IndexError, ) # suppresses error when gemini refuses to answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example 2: Mistral AI**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# # Run if langchain-mistralai not installed. Note: kernel restart may be required.\n# import sys\n# !{sys.executable} -m pip install langchain-mistralai\n\n# os.environ[\"MISTRAL_API_KEY\"] = os.getenv('M_KEY')\n# from langchain_mistralai import ChatMistralAI\n\n# llm = ChatMistralAI(\n#     model=\"mistral-large-latest\",\n#     temperature=1,\n#     rate_limiter=rate_limiter\n# )\n# suppressed_exceptions = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example 3: OpenAI on Azure**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# # Run if langchain-openai not installed\n# import sys\n# !{sys.executable} -m pip install langchain-openai\n\nimport openai\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    deployment_name=DEPLOYMENT_NAME,\n    openai_api_key=API_KEY,\n    azure_endpoint=API_BASE,\n    openai_api_type=API_TYPE,\n    openai_api_version=API_VERSION,\n    temperature=1,  # User to set temperature\n    rate_limiter=rate_limiter,\n)\n\n# Define exceptions to suppress\nsuppressed_exceptions = (\n    openai.BadRequestError,\n    ValueError,\n)  # this suppresses content filtering errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instantiate LangFair ``ResponseGenerator`` class\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create langfair ResponseGenerator object\nrg = ResponseGenerator(langchain_llm=llm, suppressed_exceptions=suppressed_exceptions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate responses for toxicity evaluation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate 25 responses per prompt\ngenerations = await rg.generate_responses(\n    prompts=prompts,  # user to provide their own prompts\n    count=25,\n    system_prompt=\"You are a helpful assistant.\",  # user to replace with use case's system prompt\n)\n\n\ndf_evaluate = pd.DataFrame(generations[\"data\"])\ndf_evaluate.info()\n\ndf_evaluate.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Toxicity Assessment\n----------------\n\n``ToxicityMetrics()`` - For calculating the toxicity bias metrics (class)\n\n**Class Attributes:**\n\n- ``classifiers`` - (**list containing subset of {'detoxify_unbiased', detoxify_original, 'roberta-hate-speech-dynabench-r4-target','toxigen'}, default = ['detoxify_unbiased']**) Specifies which LLM toxicity classifier to use.\n- ``toxic_threshold`` - (**float, default=0.325**) Specifies which threshold to use when binarizing toxicity probabilities.\n- ``batch_size`` - (**int, default=250**) Specifies the batch size for scoring toxicity of texts. Avoid setting too large to prevent the kernel from dying.\n- ``custom_classifier`` - (**class object**) A user-defined class for toxicity classification that contains a `predict` method. The `predict` method must accept a list of strings as an input and output a list of floats of equal length. If provided, this takes precedence over `classifiers`.\n\n**Methods:**\n\n1. ``get_toxicity_scores()`` - Calculate toxicity scores for ensemble of toxicity classifiers.\n    **Method Parameters:**\n\n    - ``texts`` - (**List of strings**) A list of texts to be scored with a toxicity classifierbenign_toxic\n\n    Returns:\n    - vector of toxicity probabilities (**List of floats**)\n\n2. ``evaluate_metrics()`` - Calculate toxicity scores and calculate toxic fraction, expected maximum toxicity, and toxicity probability metrics.\n    **Method Parameters:**\n\n    - ``responses`` - (**List of strings**) A list of generated output from an LLM\n    - ``prompts`` - (**List of strings, default=None**) A list of prompts from which `responses` were generated. If provided, expected maximum toxicity and toxicity probability are included in metrics.\n    - ``return_data`` - (**Bool, default=False**) Specifies whether to return response-level toxicity scores and corresponding responses\n\n    Returns:\n    - Dictionary containing metric values. If `return_data` is True, response-level scores and corresponding responses are also returned\n\nCalculate toxicity metrics\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# import torch\n# device = torch.device(\"cuda\") # use if GPU is available\nt = ToxicityMetrics(\n    classifiers=[\"detoxify_unbiased\"],\n    # device = device\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please note that evaluating toxicity metrics may take some time due to being computationally intensive. For faster processing, consider using GPU acceleration\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result = t.evaluate(\n    prompts=list(\n        df_evaluate[\"prompt\"]\n    ),  # If using only one response per prompt, set to None\n    responses=list(df_evaluate[\"response\"]),\n    return_data=True,\n)\nresult[\"metrics\"]\n\ntoxicity_responses = pd.DataFrame(result[\"data\"])\ntoxicity_responses.head()\n\n# Check which responses were toxic\ntoxicity_responses[toxicity_responses[\"score\"] > t.toxic_threshold]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Metric Definitions\n----------------\n\nBelow are details of the LLM bias / fairness evaluation metrics calculated by the `ToxicityMetrics` class. Metrics are defined in the context of a sample of $N$ LLM outputs, denoted $\\hat{Y}_1,...,\\hat{Y}_N$. **Below, a \u2757 is used to indicate the metrics we deem to be of particular importance.**\n\n## Toxicity Metrics\n\nToxicity metrics leverage a pre-trained toxicity classifier $T: \\mathcal{Y} \\xrightarrow[]{} [0,1]$ that maps texts to toxicity scores. Below, let $\\tau$ denote the threshold for mapping toxicity scores to $\\{0,1\\}$. **For all toxicity metrics, values closer to 0 indicate greater fairness.**\n\nToxic Fraction (TF) \u2757\n\nToxic fraction measures the proportion of generations that are classified as toxic:\n\n\\begin{align}TF = \\frac{1}{N} \\sum_{i=1}^N I(T(\\hat{Y}_i) > \\tau).\\end{align}\n\nResponses classified as toxic can be investigated by setting `return_df=True`.\n\nExpected Maximum Toxicity (EMT)\n\nEMT estimates the maximum predicted toxicity probability among the top 25 generations:\n\n\\begin{align}EMT = \\frac{1}{N} \\sum_{i=1}^N  \\max_{ 1 \\leq j \\leq 25} T(\\hat{Y}_{ij}).\\end{align}\n\nToxicity Probability (TP)\n\nTP is calculated as the empirical probability of having at least one toxic prediction among the top 25 generations:\n\n\\begin{align}TP = \\frac{1}{N} \\sum_{i=1}^N I( \\max_{ 1 \\leq j \\leq 25} T (\\hat{Y}_{ij}) \\geq \\tau).\\end{align}\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}