# ToxicityMetrics
@misc{gehman2020realtoxicitypromptsevaluatingneuraltoxic,
      title={RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models}, 
      author={Samuel Gehman and Suchin Gururangan and Maarten Sap and Yejin Choi and Noah A. Smith},
      year={2020},
      eprint={2009.11462},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2009.11462}, 
}

# ToxicityMetrics, StereotypeMetrics, StereotypicalAssociations
@misc{liang2023holisticevaluationlanguagemodels,
      title={Holistic Evaluation of Language Models}, 
      author={Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and Deepak Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Cosgrove and Christopher D. Manning and Christopher Ré and Diana Acosta-Navas and Drew A. Hudson and Eric Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue Wang and Keshav Santhanam and Laurel Orr and Lucia Zheng and Mert Yuksekgonul and Mirac Suzgun and Nathan Kim and Neel Guha and Niladri Chatterji and Omar Khattab and Peter Henderson and Qian Huang and Ryan Chi and Sang Michael Xie and Shibani Santurkar and Surya Ganguli and Tatsunori Hashimoto and Thomas Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda},
      year={2023},
      eprint={2211.09110},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.09110}, 
}

# StereotypeMetrics, CooccurrenceBias
@misc{bordia2019identifyingreducinggenderbias,
      title={Identifying and Reducing Gender Bias in Word-Level Language Models}, 
      author={Shikha Bordia and Samuel R. Bowman},
      year={2019},
      eprint={1904.03035},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.03035}, 
}

# StereotypeMetrics
@misc{zekun2023auditinglargelanguagemodels,
      title={Towards Auditing Large Language Models: Improving Text-based Stereotype Detection}, 
      author={Wu Zekun and Sahan Bulathwela and Adriano Soares Koshiyama},
      year={2023},
      eprint={2311.14126},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.14126}, 
}

# CounterfactualMetrics, CounterfactualSentiment
@misc{huang2020reducingsentimentbiaslanguage,
      title={Reducing Sentiment Bias in Language Models via Counterfactual Evaluation}, 
      author={Po-Sen Huang and Huan Zhang and Ray Jiang and Robert Stanforth and Johannes Welbl and Jack Rae and Vishal Maini and Dani Yogatama and Pushmeet Kohli},
      year={2020},
      eprint={1911.03064},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1911.03064}, 
}

# CounterfactualMetrics, CounterfactualSentiment
@misc{bouchard2024actionableframeworkassessingbias,
      title={An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases}, 
      author={Dylan Bouchard},
      year={2024},
      eprint={2407.10853},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10853}, 
}

# RecommendationMetrics & related classes (SERP, PRAG, Jaccard)
@inproceedings{Zhang_2023, series={RecSys ’23},
   title={Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation},
   url={http://dx.doi.org/10.1145/3604915.3608860},
   DOI={10.1145/3604915.3608860},
   booktitle={Proceedings of the 17th ACM Conference on Recommender Systems},
   publisher={ACM},
   author={Zhang, Jizhi and Bao, Keqin and Zhang, Yang and Wang, Wenjie and Feng, Fuli and He, Xiangnan},
   year={2023},
   month=sep, pages={993–999},
   collection={RecSys ’23} }

# PredictedPrevalenceDisparity, ClassificationMetrics
@misc{feldman2015certifyingremovingdisparateimpact,
      title={Certifying and removing disparate impact}, 
      author={Michael Feldman and Sorelle Friedler and John Moeller and Carlos Scheidegger and Suresh Venkatasubramanian},
      year={2015},
      eprint={1412.3756},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1412.3756}, 
}

# ClassificationMetrics, FalseNegativeRateDisparity, FalsePositiveRateDisparity, FalseOmissionRateDisparity, FalseDiscoveryRateDisparity
@misc{bellamy2018aifairness360extensible,
      title={AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias}, 
      author={Rachel K. E. Bellamy and Kuntal Dey and Michael Hind and Samuel C. Hoffman and Stephanie Houde and Kalapriya Kannan and Pranay Lohia and Jacquelyn Martino and Sameep Mehta and Aleksandra Mojsilovic and Seema Nagar and Karthikeyan Natesan Ramamurthy and John Richards and Diptikalyan Saha and Prasanna Sattigeri and Moninder Singh and Kush R. Varshney and Yunfeng Zhang},
      year={2018},
      eprint={1810.01943},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1810.01943}, 
}

# ClassificationMetrics, FalseNegativeRateDisparity, FalsePositiveRateDisparity, FalseOmissionRateDisparity, FalseDiscoveryRateDisparity
@misc{saleiro2019aequitasbiasfairnessaudit,
      title={Aequitas: A Bias and Fairness Audit Toolkit}, 
      author={Pedro Saleiro and Benedict Kuester and Loren Hinkson and Jesse London and Abby Stevens and Ari Anisfeld and Kit T. Rodolfa and Rayid Ghani},
      year={2019},
      eprint={1811.05577},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1811.05577}, 
}

