
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/toxicity_metrics_demo.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_toxicity_metrics_demo.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_toxicity_metrics_demo.py:


.. _toxicity_metrics:

===============================================================
Toxicity Metrics
===============================================================

.. GENERATED FROM PYTHON SOURCE LINES 10-25

.. warning::
  Due to the topic of bias and fairness, some users may be offended by the content contained herein, including prompts and output generated from use of the prompts.

Content
*******

1. :ref:`Introduction<intro>`

2. :ref:`Generate Evaluation Dataset<gen-evaluation-dataset>`

3. :ref:`Assessment<assessment>`

4. :ref:`Metric Definitions<metric-defns>`

Import necessary libraries for the notebook.

.. GENERATED FROM PYTHON SOURCE LINES 25-52

.. code-block:: Python


    # Run if python-dotenv not installed
    # import sys
    # !{sys.executable} -m pip install python-dotenv

    import json
    import os

    import pandas as pd
    from dotenv import find_dotenv, load_dotenv
    from langchain_core.rate_limiters import InMemoryRateLimiter

    from langfair.generator import ResponseGenerator
    from langfair.metrics.toxicity import ToxicityMetrics

    # User to populate .env file with API credentials
    repo_path = "/".join(os.getcwd().split("/")[:-3])
    load_dotenv(find_dotenv())

    API_KEY = os.getenv("API_KEY")
    API_BASE = os.getenv("API_BASE")
    API_TYPE = os.getenv("API_TYPE")
    API_VERSION = os.getenv("API_VERSION")
    MODEL_VERSION = os.getenv("MODEL_VERSION")
    DEPLOYMENT_NAME = os.getenv("DEPLOYMENT_NAME")



.. GENERATED FROM PYTHON SOURCE LINES 53-74

.. _intro:
1. Introduction
----------------

Toxicity in large language model (LLM) outputs refers to offensive language that 1) launches attacks, issues threats, or
incites hate or violence against a social group, or 2) includes the usage of pejorative slurs, insults, or any other forms of
expression that specifically target and belittle a social group. LangFair offers the following toxicity metrics from the LLM fairness literature:

* Expected Maximum Toxicity `Gehman et al., 2020 <https://arxiv.org/pdf/2009.11462>`_
* Toxicity Probability `Gehman et al., 2020 <https://arxiv.org/pdf/2009.11462>`_
* Toxic Fraction `Liang et al., 2023 <https://arxiv.org/pdf/2211.09110>`_

For more details on the definitions of these metrics, refer to the [metric definitions](#section4') in this notebook or LangFair's `technical playbook <https://arxiv.org/pdf/2407.10853>`_.

.. _gen-evaluation-dataset:
2. Generate Evaluation Dataset (skip if responses already generated)
----------------

Here, we will use ``ResponseGenerator`` to generate a large sample of responses with our LLM of choice. The user should replace our example prompts with actual prompts from their use case. **If the user already has a large number of responses generated, they may skip this step.**

Read in prompts

.. GENERATED FROM PYTHON SOURCE LINES 74-87

.. code-block:: Python


    # THIS IS AN EXAMPLE SET OF PROMPTS. USER TO REPLACE WITH THEIR OWN PROMPTS
    resource_path = os.path.join(repo_path, "data/RealToxicityPrompts.jsonl")
    with open(resource_path, "r") as file:
        # Read each line in the file
        challenging = []
        prompts = []
        for line in file:
            # Parse the JSON object from each line
            challenging.append(json.loads(line)["challenging"])
            prompts.append(json.loads(line)["prompt"]["text"])
    prompts = [prompts[i] for i in range(len(prompts)) if not challenging[i]][0:100]


.. GENERATED FROM PYTHON SOURCE LINES 88-117

Note that sample size is intentionally kept low to reduce execution time of this notebook. User should use all the available propmpts and can use `ResponseGenerator` class to generate more response from a model.

Evaluation Dataset Generation

``ResponseGenerator()`` - Class for generating data for evaluation from provided set of prompts (class)

Class parameters:

  - ``langchain_llm`` (**langchain llm (Runnable), default=None**) A langchain llm object to get passed to LLMChain `llm` argument.
  - ``suppressed_exceptions`` (**tuple, default=None**) Specifies which exceptions to handle as 'Unable to get response' rather than raising the exception
  - ``max_calls_per_min`` (**Deprecated as of 0.2.0**) Use LangChain's InMemoryRateLimiter instead.

Methods:

``generate_responses()`` -  Generates evaluation dataset from a provided set of prompts. For each prompt, `self.count` responses are generated.
Method Parameters:

- ``prompts`` - (**list of strings**) A list of prompts
- ``system_prompt`` - (**str or None, default="You are a helpful assistant."**) Specifies the system prompt used when generating LLM responses.
- ``count`` - (**int, default=25**) Specifies number of responses to generate for each prompt.

Returns:
A dictionary with two keys: ``data`` and ``metadata``.
- ``data`` (**dict**) A dictionary containing the prompts and responses.
- ``metadata`` (**dict**) A dictionary containing metadata about the generation process, including non-completion rate, temperature, and count.

Below we use LangFair's `ResponseGenerator` class to generate LLM responses, which will be used to compute evaluation metrics. To instantiate the `ResponseGenerator` class, pass a LangChain LLM object as an argument.

**Important note: We provide three examples of LangChain LLMs below, but these can be replaced with a LangChain LLM of your choice.**

.. GENERATED FROM PYTHON SOURCE LINES 117-125

.. code-block:: Python


    # Use LangChain's InMemoryRateLimiter to avoid rate limit errors. Adjust parameters as necessary.
    rate_limiter = InMemoryRateLimiter(
        requests_per_second=10,
        check_every_n_seconds=10,
        max_bucket_size=1000,
    )


.. GENERATED FROM PYTHON SOURCE LINES 126-127

**Example 1: Gemini Pro with VertexAI**

.. GENERATED FROM PYTHON SOURCE LINES 127-138

.. code-block:: Python


    # # Run if langchain-google-vertexai not installed. Note: kernel restart may be required.
    # import sys
    # !{sys.executable} -m pip install langchain-google-vertexai

    # from langchain_google_vertexai import VertexAI
    # llm = VertexAI(model_name='gemini-pro', temperature=1, rate_limiter=rate_limiter)

    # # Define exceptions to suppress
    # suppressed_exceptions = (IndexError, ) # suppresses error when gemini refuses to answer


.. GENERATED FROM PYTHON SOURCE LINES 139-140

**Example 2: Mistral AI**

.. GENERATED FROM PYTHON SOURCE LINES 140-155

.. code-block:: Python


    # # Run if langchain-mistralai not installed. Note: kernel restart may be required.
    # import sys
    # !{sys.executable} -m pip install langchain-mistralai

    # os.environ["MISTRAL_API_KEY"] = os.getenv('M_KEY')
    # from langchain_mistralai import ChatMistralAI

    # llm = ChatMistralAI(
    #     model="mistral-large-latest",
    #     temperature=1,
    #     rate_limiter=rate_limiter
    # )
    # suppressed_exceptions = None


.. GENERATED FROM PYTHON SOURCE LINES 156-157

**Example 3: OpenAI on Azure**

.. GENERATED FROM PYTHON SOURCE LINES 157-181

.. code-block:: Python


    # # Run if langchain-openai not installed
    # import sys
    # !{sys.executable} -m pip install langchain-openai

    import openai
    from langchain_openai import AzureChatOpenAI

    llm = AzureChatOpenAI(
        deployment_name=DEPLOYMENT_NAME,
        openai_api_key=API_KEY,
        azure_endpoint=API_BASE,
        openai_api_type=API_TYPE,
        openai_api_version=API_VERSION,
        temperature=1,  # User to set temperature
        rate_limiter=rate_limiter,
    )

    # Define exceptions to suppress
    suppressed_exceptions = (
        openai.BadRequestError,
        ValueError,
    )  # this suppresses content filtering errors


.. GENERATED FROM PYTHON SOURCE LINES 182-183

Instantiate LangFair ``ResponseGenerator`` class

.. GENERATED FROM PYTHON SOURCE LINES 183-187

.. code-block:: Python


    # Create langfair ResponseGenerator object
    rg = ResponseGenerator(langchain_llm=llm, suppressed_exceptions=suppressed_exceptions)


.. GENERATED FROM PYTHON SOURCE LINES 188-189

Generate responses for toxicity evaluation

.. GENERATED FROM PYTHON SOURCE LINES 189-204

.. code-block:: Python


    # Generate 25 responses per prompt
    generations = await rg.generate_responses(
        prompts=prompts,  # user to provide their own prompts
        count=25,
        system_prompt="You are a helpful assistant.",  # user to replace with use case's system prompt
    )


    df_evaluate = pd.DataFrame(generations["data"])
    df_evaluate.info()

    df_evaluate.head()



.. GENERATED FROM PYTHON SOURCE LINES 205-239

.. _assessment:
3. Toxicity Assessment
----------------

``ToxicityMetrics()`` - For calculating the toxicity bias metrics (class)

**Class Attributes:**

- ``classifiers`` - (**list containing subset of {'detoxify_unbiased', detoxify_original, 'roberta-hate-speech-dynabench-r4-target','toxigen'}, default = ['detoxify_unbiased']**) Specifies which LLM toxicity classifier to use.
- ``toxic_threshold`` - (**float, default=0.325**) Specifies which threshold to use when binarizing toxicity probabilities.
- ``batch_size`` - (**int, default=250**) Specifies the batch size for scoring toxicity of texts. Avoid setting too large to prevent the kernel from dying.
- ``custom_classifier`` - (**class object**) A user-defined class for toxicity classification that contains a `predict` method. The `predict` method must accept a list of strings as an input and output a list of floats of equal length. If provided, this takes precedence over `classifiers`.

**Methods:**

1. ``get_toxicity_scores()`` - Calculate toxicity scores for ensemble of toxicity classifiers.
    **Method Parameters:**

    - ``texts`` - (**List of strings**) A list of texts to be scored with a toxicity classifierbenign_toxic

    Returns:
    - vector of toxicity probabilities (**List of floats**)

2. ``evaluate_metrics()`` - Calculate toxicity scores and calculate toxic fraction, expected maximum toxicity, and toxicity probability metrics.
    **Method Parameters:**

    - ``responses`` - (**List of strings**) A list of generated output from an LLM
    - ``prompts`` - (**List of strings, default=None**) A list of prompts from which `responses` were generated. If provided, expected maximum toxicity and toxicity probability are included in metrics.
    - ``return_data`` - (**Bool, default=False**) Specifies whether to return response-level toxicity scores and corresponding responses

    Returns:
    - Dictionary containing metric values. If `return_data` is True, response-level scores and corresponding responses are also returned

Calculate toxicity metrics

.. GENERATED FROM PYTHON SOURCE LINES 239-247

.. code-block:: Python


    # import torch
    # device = torch.device("cuda") # use if GPU is available
    t = ToxicityMetrics(
        classifiers=["detoxify_unbiased"],
        # device = device
    )


.. GENERATED FROM PYTHON SOURCE LINES 248-249

Please note that evaluating toxicity metrics may take some time due to being computationally intensive. For faster processing, consider using GPU acceleration

.. GENERATED FROM PYTHON SOURCE LINES 249-266

.. code-block:: Python


    result = t.evaluate(
        prompts=list(
            df_evaluate["prompt"]
        ),  # If using only one response per prompt, set to None
        responses=list(df_evaluate["response"]),
        return_data=True,
    )
    result["metrics"]

    toxicity_responses = pd.DataFrame(result["data"])
    toxicity_responses.head()

    # Check which responses were toxic
    toxicity_responses[toxicity_responses["score"] > t.toxic_threshold]



.. GENERATED FROM PYTHON SOURCE LINES 267-300

.. _metric-defns:
4. Metric Definitions
----------------

Below are details of the LLM bias / fairness evaluation metrics calculated by the `ToxicityMetrics` class. Metrics are defined in the context of a sample of :math:`N` LLM outputs, denoted :math:`\hat{Y}_1,...,\hat{Y}_N`. **Below, a ❗ is used to indicate the metrics we deem to be of particular importance.**

Toxicity Metrics
****************

Toxicity metrics leverage a pre-trained toxicity classifier :math:`T: \mathcal{Y} \xrightarrow[]{} [0,1]` that maps texts to toxicity scores. Below, let :math:`\tau` denote the threshold for mapping toxicity scores to :math:`\{0,1\}`. **For all toxicity metrics, values closer to 0 indicate greater fairness.**

Toxic Fraction (TF) ❗

Toxic fraction measures the proportion of generations that are classified as toxic:

.. math::
  TF = \frac{1}{N} \sum_{i=1}^N I(T(\hat{Y}_i) > \tau).

Responses classified as toxic can be investigated by setting `return_df=True`.

Expected Maximum Toxicity (EMT)

EMT estimates the maximum predicted toxicity probability among the top 25 generations:

.. math::
  EMT = \frac{1}{N} \sum_{i=1}^N  \max_{ 1 \leq j \leq 25} T(\hat{Y}_{ij}).

Toxicity Probability (TP)

TP is calculated as the empirical probability of having at least one toxic prediction among the top 25 generations:

.. math::
  TP = \frac{1}{N} \sum_{i=1}^N I( \max_{ 1 \leq j \leq 25} T (\hat{Y}_{ij}) \geq \tau).


.. _sphx_glr_download_auto_examples_toxicity_metrics_demo.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: toxicity_metrics_demo.ipynb <toxicity_metrics_demo.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: toxicity_metrics_demo.py <toxicity_metrics_demo.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: toxicity_metrics_demo.zip <toxicity_metrics_demo.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
