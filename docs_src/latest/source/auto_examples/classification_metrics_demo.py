"""
.. _classification_metrics_demo:

===============================================================
Classification Metrics
===============================================================

"""

import numpy as np

from langfair.metrics.classification import ClassificationMetrics

# %%
# 1. Introduction
# ---------------
#
# Large language models (LLMs) used in classification use cases should be assessed for group fairness (if applicable). Similar to traditional person-level classification challenges in machine learning, these use cases present the risk of allocational harms.  LangFair offers the following classification fairness metrics from the LLM fairness literature:
#
# * Predicted Prevalence Rate Disparity `Feldman et al., 2015 <https://arxiv.org/abs/1412.3756>`_ , `Bellamy et al., 2018 <https://arxiv.org/abs/1810.01943>`_ , `Saleiro et al., 2019 <https://arxiv.org/abs/1811.05577>`_
# * False Negative Rate Disparity `Bellamy et al., 2018 <https://arxiv.org/abs/1810.01943>`_ , `Saleiro et al., 2019 <https://arxiv.org/abs/1811.05577>`_
# * False Omission Rate Disparity `Bellamy et al., 2018 <https://arxiv.org/abs/1810.01943>`_ , `Saleiro et al., 2019 <https://arxiv.org/abs/1811.05577>`_
# * False Positive Rate Disparity `Bellamy et al., 2018 <https://arxiv.org/abs/1810.01943>`_ , `Saleiro et al., 2019 <https://arxiv.org/abs/1811.05577>`_
# * False Discovery Rate Disparity `Bellamy et al., 2018 <https://arxiv.org/abs/1810.01943>`_ , `Saleiro et al., 2019 <https://arxiv.org/abs/1811.05577>`_
# 2. Assessment
# -------------


# Simulate dataset for this example. In practice, users should replace this data with predicted classes generated by the LLM,
# corresponding ground truth values, and corresponding protected attribute group data.
sample_size = 10000
groups = np.random.binomial(n=1, p=0.5, size=sample_size)
y_pred = np.random.binomial(n=1, p=0.3, size=sample_size)
y_true = np.random.binomial(n=1, p=0.3, size=sample_size)

# %%
# Classification Metrics
# ----------------------
# ``ClassificationMetrics()`` - For calculating FaiRLLM (Fairness of Recommendation via LLM) metrics (class)
#
# **Class parameters:**
#
#   - ``metric_type`` - (**{'all', 'assistive', 'punitive', 'representation'}, default='all'**) Specifies which metrics to use.
#
# **Methods:**
#
# 1. ``evaluate`` - Returns min, max, range, and standard deviation of metrics across protected attribute groups.
#
#   **Method Parameters:**
#     - ``groups`` - (**array-like**) Group indicators. Must contain exactly two unique values.
#     - ``y_pred`` - (**array-like**) Binary model predictions. Positive and negative predictions must be 1 and 0, respectively.
#     - ``y_true`` - (**array-like**) Binary labels (ground truth values). Positive and negative labels must be 1 and 0, respectively.
#     - ``ratio`` - (**boolean**) Indicates whether to compute the metric as a difference or a ratio
#
#     Returns:
#     - Dictionary containing fairness metric values (**Dictionary**).
#
# Generate an instance of class ``ClassificationMetrics`` using default ``metric_type='all'``, which includes "assistive", "punitive", and "representation" metrics.

cm = ClassificationMetrics(metric_type="all")

# Metrics expressed as ratios (target value of 1)
cm.evaluate(groups=groups, y_pred=y_pred, y_true=y_true, ratio=True)

# Metrics expressed as differences (target value of 0)
cm.evaluate(groups=groups, y_pred=y_pred, y_true=y_true, ratio=False)
