
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/classification_metrics_demo.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_classification_metrics_demo.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_classification_metrics_demo.py:


.. _classification_metrics_demo:

===============================================================
Classification Metrics
===============================================================

.. GENERATED FROM PYTHON SOURCE LINES 9-14

.. code-block:: Python


    import numpy as np

    from langfair.metrics.classification import ClassificationMetrics


.. GENERATED FROM PYTHON SOURCE LINES 15-27

1. Introduction
---------------

Large language models (LLMs) used in classification use cases should be assessed for group fairness (if applicable). Similar to traditional person-level classification challenges in machine learning, these use cases present the risk of allocational harms.  LangFair offers the following classification fairness metrics from the LLM fairness literature:

* Predicted Prevalence Rate Disparity `Feldman et al., 2015 <https://arxiv.org/abs/1412.3756>`_ , `Bellamy et al., 2018 <https://arxiv.org/abs/1810.01943>`_ , `Saleiro et al., 2019 <https://arxiv.org/abs/1811.05577>`_
* False Negative Rate Disparity `Bellamy et al., 2018 <https://arxiv.org/abs/1810.01943>`_ , `Saleiro et al., 2019 <https://arxiv.org/abs/1811.05577>`_
* False Omission Rate Disparity `Bellamy et al., 2018 <https://arxiv.org/abs/1810.01943>`_ , `Saleiro et al., 2019 <https://arxiv.org/abs/1811.05577>`_
* False Positive Rate Disparity `Bellamy et al., 2018 <https://arxiv.org/abs/1810.01943>`_ , `Saleiro et al., 2019 <https://arxiv.org/abs/1811.05577>`_
* False Discovery Rate Disparity `Bellamy et al., 2018 <https://arxiv.org/abs/1810.01943>`_ , `Saleiro et al., 2019 <https://arxiv.org/abs/1811.05577>`_
2. Assessment
-------------

.. GENERATED FROM PYTHON SOURCE LINES 27-36

.. code-block:: Python



    # Simulate dataset for this example. In practice, users should replace this data with predicted classes generated by the LLM,
    # corresponding ground truth values, and corresponding protected attribute group data.
    sample_size = 10000
    groups = np.random.binomial(n=1, p=0.5, size=sample_size)
    y_pred = np.random.binomial(n=1, p=0.3, size=sample_size)
    y_true = np.random.binomial(n=1, p=0.3, size=sample_size)


.. GENERATED FROM PYTHON SOURCE LINES 37-59

Classification Metrics
----------------------
``ClassificationMetrics()`` - For calculating FaiRLLM (Fairness of Recommendation via LLM) metrics (class)

**Class parameters:**

  - ``metric_type`` - (**{'all', 'assistive', 'punitive', 'representation'}, default='all'**) Specifies which metrics to use.

**Methods:**

1. ``evaluate`` - Returns min, max, range, and standard deviation of metrics across protected attribute groups.

  **Method Parameters:**
    - ``groups`` - (**array-like**) Group indicators. Must contain exactly two unique values.
    - ``y_pred`` - (**array-like**) Binary model predictions. Positive and negative predictions must be 1 and 0, respectively.
    - ``y_true`` - (**array-like**) Binary labels (ground truth values). Positive and negative labels must be 1 and 0, respectively.
    - ``ratio`` - (**boolean**) Indicates whether to compute the metric as a difference or a ratio

    Returns:
    - Dictionary containing fairness metric values (**Dictionary**).

Generate an instance of class ``ClassificationMetrics`` using default ``metric_type='all'``, which includes "assistive", "punitive", and "representation" metrics.

.. GENERATED FROM PYTHON SOURCE LINES 59-67

.. code-block:: Python


    cm = ClassificationMetrics(metric_type="all")

    # Metrics expressed as ratios (target value of 1)
    cm.evaluate(groups=groups, y_pred=y_pred, y_true=y_true, ratio=True)

    # Metrics expressed as differences (target value of 0)
    cm.evaluate(groups=groups, y_pred=y_pred, y_true=y_true, ratio=False)


.. _sphx_glr_download_auto_examples_classification_metrics_demo.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: classification_metrics_demo.ipynb <classification_metrics_demo.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: classification_metrics_demo.py <classification_metrics_demo.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: classification_metrics_demo.zip <classification_metrics_demo.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
